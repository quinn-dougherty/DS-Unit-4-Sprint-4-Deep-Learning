{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhU3R-8dzk5z"
   },
   "source": [
    "# Lambda School Data Science Unit 4 Sprint Challenge 4\n",
    "\n",
    "## RNNs, CNNs, AutoML, and more...\n",
    "\n",
    "In this sprint challenge, you'll explore some of the cutting edge of Data Science.\n",
    "\n",
    "*Caution* - these approaches can be pretty heavy computationally. All problems were designed so that you should be able to achieve results within at most 5-10 minutes of runtime on Colab or a comparable environment. If something is running longer, doublecheck your approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5UwGRnJOmD4"
   },
   "source": [
    "## Part 1 - RNNs\n",
    "\n",
    "Use an RNN to fit a simple classification model on tweets to distinguish from tweets from Austen Allred and tweets from Weird Al Yankovic.\n",
    "\n",
    "Following is code to scrape the needed data (no API auth needed, uses [twitterscraper](https://github.com/taspinar/twitterscraper)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "3if1yTMUoG3U",
    "outputId": "6075f571-d8b1-4e91-bfb9-26ad7a467261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twitterscraper\n",
      "  Downloading https://files.pythonhosted.org/packages/38/7d/0bf84247b78d7d223914cbf410e1160203a65d39086aaf8c6cad521cec74/twitterscraper-0.9.3.tar.gz\n",
      "Collecting coala-utils~=0.5.0 (from twitterscraper)\n",
      "  Downloading https://files.pythonhosted.org/packages/54/00/74ec750cfc4e830f9d1cfdd4d559f3d2d4ba1b834b78d5266446db3fd1d6/coala_utils-0.5.1-py3-none-any.whl\n",
      "Collecting bs4 (from twitterscraper)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: lxml in /home/quinn/anaconda3/lib/python3.6/site-packages (from twitterscraper) (4.3.0)\n",
      "Requirement already satisfied: requests in /home/quinn/.local/lib/python3.6/site-packages (from twitterscraper) (2.20.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/quinn/anaconda3/lib/python3.6/site-packages (from bs4->twitterscraper) (4.7.1)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/quinn/.local/lib/python3.6/site-packages (from requests->twitterscraper) (2.7)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/quinn/anaconda3/lib/python3.6/site-packages (from requests->twitterscraper) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/quinn/.local/lib/python3.6/site-packages (from requests->twitterscraper) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/quinn/.local/lib/python3.6/site-packages (from requests->twitterscraper) (2018.10.15)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/quinn/anaconda3/lib/python3.6/site-packages (from beautifulsoup4->bs4->twitterscraper) (1.7.1)\n",
      "Building wheels for collected packages: twitterscraper, bs4\n",
      "  Running setup.py bdist_wheel for twitterscraper ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/quinn/.cache/pip/wheels/45/50/9b/70128bca07e2bf8b5ed3f504002e9e74a6eaa5e756341b6931\n",
      "  Running setup.py bdist_wheel for bs4 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/quinn/.cache/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built twitterscraper bs4\n",
      "Installing collected packages: coala-utils, bs4, twitterscraper\n",
      "Successfully installed bs4-0.0.1 coala-utils-0.5.1 twitterscraper-0.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install twitterscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1114
    },
    "colab_type": "code",
    "id": "DS-9ksWjoJit",
    "outputId": "0c3512e4-5cd4-4dc6-9cda-baf00c835f59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:austen since:2006-03-21 until:2006-11-14', 'from:austen since:2006-11-14 until:2007-07-11', 'from:austen since:2007-07-11 until:2008-03-05', 'from:austen since:2008-03-05 until:2008-10-30', 'from:austen since:2008-10-30 until:2009-06-25', 'from:austen since:2009-06-25 until:2010-02-19', 'from:austen since:2010-02-19 until:2010-10-15', 'from:austen since:2010-10-15 until:2011-06-11', 'from:austen since:2011-06-11 until:2012-02-04', 'from:austen since:2012-02-04 until:2012-09-30', 'from:austen since:2012-09-30 until:2013-05-26', 'from:austen since:2013-05-26 until:2014-01-20', 'from:austen since:2014-01-20 until:2014-09-15', 'from:austen since:2014-09-15 until:2015-05-12', 'from:austen since:2015-05-12 until:2016-01-05', 'from:austen since:2016-01-05 until:2016-08-31', 'from:austen since:2016-08-31 until:2017-04-26', 'from:austen since:2017-04-26 until:2017-12-21', 'from:austen since:2017-12-21 until:2018-08-16', 'from:austen since:2018-08-16 until:2019-04-12']\n",
      "INFO: Querying from:austen since:2007-07-11 until:2008-03-05\n",
      "INFO: Querying from:austen since:2006-11-14 until:2007-07-11\n",
      "INFO: Querying from:austen since:2006-03-21 until:2006-11-14\n",
      "INFO: Querying from:austen since:2009-06-25 until:2010-02-19\n",
      "INFO: Querying from:austen since:2011-06-11 until:2012-02-04\n",
      "INFO: Querying from:austen since:2010-02-19 until:2010-10-15\n",
      "INFO: Querying from:austen since:2010-10-15 until:2011-06-11\n",
      "INFO: Querying from:austen since:2008-03-05 until:2008-10-30\n",
      "INFO: Querying from:austen since:2015-05-12 until:2016-01-05\n",
      "INFO: Querying from:austen since:2014-09-15 until:2015-05-12\n",
      "INFO: Querying from:austen since:2017-12-21 until:2018-08-16\n",
      "INFO: Querying from:austen since:2017-04-26 until:2017-12-21\n",
      "INFO: Querying from:austen since:2018-08-16 until:2019-04-12\n",
      "INFO: Querying from:austen since:2008-10-30 until:2009-06-25\n",
      "INFO: Querying from:austen since:2012-02-04 until:2012-09-30\n",
      "INFO: Querying from:austen since:2012-09-30 until:2013-05-26\n",
      "INFO: Querying from:austen since:2016-01-05 until:2016-08-31\n",
      "INFO: Querying from:austen since:2013-05-26 until:2014-01-20\n",
      "INFO: Querying from:austen since:2016-08-31 until:2017-04-26\n",
      "INFO: Querying from:austen since:2014-01-20 until:2014-09-15\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2007-07-11%20until%3A2008-03-05.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2010-10-15%20until%3A2011-06-11.\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2016-08-31%20until%3A2017-04-26.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2014-09-15%20until%3A2015-05-12.\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2008-10-30%20until%3A2009-06-25.\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2016-01-05%20until%3A2016-08-31.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2006-03-21%20until%3A2006-11-14.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2014-01-20%20until%3A2014-09-15.\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2012-09-30%20until%3A2013-05-26.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2015-05-12%20until%3A2016-01-05.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2013-05-26%20until%3A2014-01-20.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2010-02-19%20until%3A2010-10-15.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2008-03-05%20until%3A2008-10-30.\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2009-06-25%20until%3A2010-02-19.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2006-11-14%20until%3A2007-07-11.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2011-06-11%20until%3A2012-02-04.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 1 tweets for from%3Aausten%20since%3A2012-02-04%20until%3A2012-09-30.\n",
      "INFO: Got 1 tweets (1 new).\n",
      "INFO: Got 60 tweets for from%3Aausten%20since%3A2018-08-16%20until%3A2019-04-12.\n",
      "INFO: Got 60 tweets for from%3Aausten%20since%3A2017-04-26%20until%3A2017-12-21.\n",
      "INFO: Got 61 tweets (60 new).\n",
      "INFO: Got 121 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aausten%20since%3A2017-12-21%20until%3A2018-08-16.\n",
      "INFO: Got 181 tweets (60 new).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "\n",
    "austen_tweets = query_tweets('from:austen', 1000)\n",
    "len(austen_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fLKqFh8DovaN",
    "outputId": "64b0d621-7e74-4181-9116-406e8c518465"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love love love working with great people.pic.twitter.com/fCKOm6Vl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1114
    },
    "colab_type": "code",
    "id": "MRQeIIf1orCS",
    "outputId": "44b57b5e-2a0e-4656-ca06-d77637caf593"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:AlYankovic since:2006-03-21 until:2006-11-14', 'from:AlYankovic since:2006-11-14 until:2007-07-11', 'from:AlYankovic since:2007-07-11 until:2008-03-05', 'from:AlYankovic since:2008-03-05 until:2008-10-30', 'from:AlYankovic since:2008-10-30 until:2009-06-25', 'from:AlYankovic since:2009-06-25 until:2010-02-19', 'from:AlYankovic since:2010-02-19 until:2010-10-15', 'from:AlYankovic since:2010-10-15 until:2011-06-11', 'from:AlYankovic since:2011-06-11 until:2012-02-04', 'from:AlYankovic since:2012-02-04 until:2012-09-30', 'from:AlYankovic since:2012-09-30 until:2013-05-26', 'from:AlYankovic since:2013-05-26 until:2014-01-20', 'from:AlYankovic since:2014-01-20 until:2014-09-15', 'from:AlYankovic since:2014-09-15 until:2015-05-12', 'from:AlYankovic since:2015-05-12 until:2016-01-05', 'from:AlYankovic since:2016-01-05 until:2016-08-31', 'from:AlYankovic since:2016-08-31 until:2017-04-26', 'from:AlYankovic since:2017-04-26 until:2017-12-21', 'from:AlYankovic since:2017-12-21 until:2018-08-16', 'from:AlYankovic since:2018-08-16 until:2019-04-12']\n",
      "INFO: Querying from:AlYankovic since:2006-03-21 until:2006-11-14\n",
      "INFO: Querying from:AlYankovic since:2006-11-14 until:2007-07-11\n",
      "INFO: Querying from:AlYankovic since:2007-07-11 until:2008-03-05\n",
      "INFO: Querying from:AlYankovic since:2009-06-25 until:2010-02-19\n",
      "INFO: Querying from:AlYankovic since:2008-03-05 until:2008-10-30\n",
      "INFO: Querying from:AlYankovic since:2008-10-30 until:2009-06-25\n",
      "INFO: Querying from:AlYankovic since:2010-10-15 until:2011-06-11\n",
      "INFO: Querying from:AlYankovic since:2010-02-19 until:2010-10-15\n",
      "INFO: Querying from:AlYankovic since:2017-04-26 until:2017-12-21\n",
      "INFO: Querying from:AlYankovic since:2015-05-12 until:2016-01-05\n",
      "INFO: Querying from:AlYankovic since:2017-12-21 until:2018-08-16\n",
      "INFO: Querying from:AlYankovic since:2018-08-16 until:2019-04-12\n",
      "INFO: Querying from:AlYankovic since:2016-08-31 until:2017-04-26\n",
      "INFO: Querying from:AlYankovic since:2014-01-20 until:2014-09-15\n",
      "INFO: Querying from:AlYankovic since:2014-09-15 until:2015-05-12\n",
      "INFO: Querying from:AlYankovic since:2016-01-05 until:2016-08-31\n",
      "INFO: Querying from:AlYankovic since:2011-06-11 until:2012-02-04\n",
      "INFO: Querying from:AlYankovic since:2012-02-04 until:2012-09-30\n",
      "INFO: Querying from:AlYankovic since:2012-09-30 until:2013-05-26\n",
      "INFO: Querying from:AlYankovic since:2013-05-26 until:2014-01-20\n",
      "INFO: Got 0 tweets for from%3AAlYankovic%20since%3A2006-11-14%20until%3A2007-07-11.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3AAlYankovic%20since%3A2007-07-11%20until%3A2008-03-05.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3AAlYankovic%20since%3A2008-03-05%20until%3A2008-10-30.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3AAlYankovic%20since%3A2006-03-21%20until%3A2006-11-14.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2010-10-15%20until%3A2011-06-11.\n",
      "INFO: Got 60 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2012-02-04%20until%3A2012-09-30.\n",
      "INFO: Got 120 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2008-10-30%20until%3A2009-06-25.\n",
      "INFO: Got 180 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2010-02-19%20until%3A2010-10-15.\n",
      "INFO: Got 240 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2015-05-12%20until%3A2016-01-05.\n",
      "INFO: Got 300 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2014-09-15%20until%3A2015-05-12.\n",
      "INFO: Got 360 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2018-08-16%20until%3A2019-04-12.\n",
      "INFO: Got 420 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2014-01-20%20until%3A2014-09-15.\n",
      "INFO: Got 480 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2016-08-31%20until%3A2017-04-26.\n",
      "INFO: Got 540 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2009-06-25%20until%3A2010-02-19.\n",
      "INFO: Got 600 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2011-06-11%20until%3A2012-02-04.\n",
      "INFO: Got 660 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2017-04-26%20until%3A2017-12-21.\n",
      "INFO: Got 720 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2017-12-21%20until%3A2018-08-16.\n",
      "INFO: Got 780 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2012-09-30%20until%3A2013-05-26.\n",
      "INFO: Got 840 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2013-05-26%20until%3A2014-01-20.\n",
      "INFO: Got 900 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2016-01-05%20until%3A2016-08-31.\n",
      "INFO: Got 960 tweets (60 new).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al_tweets = query_tweets('from:AlYankovic', 1000)\n",
    "len(al_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_dB7I87ty8f1",
    "outputId": "38e7dffb-92bb-4a4c-8bbd-f1ddd951cc85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well well well... look what just showed up on my doorstep! http://twitpic.com/59mi2c'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0mrcjEu_zRl4",
    "outputId": "cdea5ac9-bf26-434f-d2aa-fbc251c8ceef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(austen_tweets + al_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYCVJX6ep8iO"
   },
   "source": [
    "Your tasks:\n",
    "\n",
    "- Encode the characters to a sequence of integers for the model\n",
    "- Get the data into the appropriate shape/format, including labels and a train/test split\n",
    "- Use Keras to fit a predictive model, classifying tweets as being from Austen versus Weird Al\n",
    "- Report your overall score and accuracy\n",
    "\n",
    "For reference, the [Keras IMDB sentiment classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well the RNN code we used in class.\n",
    "\n",
    "*Note* - focus on getting a running model, not on maxing accuracy with extreme data size or epoch numbers. Only revisit and push accuracy if you get everything else done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(570, 80) (570, 80) (570, 80) (570,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# max_features = 20000\n",
    "# # cut texts after this number of words (among top max_features most common words)\n",
    "# maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def load_tweet_dat(seq_length=80): \n",
    "    \n",
    "    rp = np.random.permutation(austen_tweets + al_tweets)\n",
    "    tweet = [t.text for t in rp]\n",
    "    name = [t.user for t in rp]\n",
    "    \n",
    "    seqs = [[ord(c) for c in x] for x in [t.text for t in rp]]\n",
    "    \n",
    "    aust = sequence.pad_sequences(np.array([[ord(c) for c in t.text] for t in austen_tweets]), maxlen=80)\n",
    "    weal = sequence.pad_sequences(np.array([[ord(c) for c in t.text] for t in al_tweets]), maxlen=80)\n",
    "\n",
    "    aust_df = pd.DataFrame(aust).assign(by_user=[t.user for t in austen_tweets])\n",
    "    weal_df = pd.DataFrame(weal).assign(by_user=[t.user for t in al_tweets]).drop(0, axis=0)\n",
    "\n",
    "    df = pd.concat([aust_df, weal_df]).sample(frac=1).reset_index(drop=True)\n",
    "    return train_test_split(df.drop(['by_user'], axis=1), df.by_user.map({'alyankovic': 1, 'austen': 0}).fillna(0), test_size=0.5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_tweet_dat()\n",
    "\n",
    "print(X_train.shape, X_test.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[21,47] = 8217 is not in [0, 570)\n\t [[Node: embedding_11/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training_4/Adam/Assign_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_11/embeddings/read, embedding_11/Cast, training_4/Adam/gradients/embedding_11/embedding_lookup_grad/concat/axis)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-e29a4dee71b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m           validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m     17\u001b[0m score, acc = model.evaluate(X_test, y_test,\n\u001b[1;32m     18\u001b[0m                             batch_size=batch_size)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[21,47] = 8217 is not in [0, 570)\n\t [[Node: embedding_11/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training_4/Adam/Assign_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_11/embeddings/read, embedding_11/Cast, training_4/Adam/gradients/embedding_11/embedding_lookup_grad/concat/axis)]]"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(570, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QVSlFEAqWJM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPn6c0x21gu1"
   },
   "source": [
    "Conclusion - RNN runs, and gives pretty decent improvement over a naive \"It's Al!\" model. To *really* improve the model, more playing with parameters, and just getting more data (particularly Austen tweets), would help. Also - RNN may well not be the best approach here, but it is at least a valid one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yz0LCZd_O4IG"
   },
   "source": [
    "## Part 2- CNNs\n",
    "\n",
    "Time to play \"find the frog!\" Use Keras and ResNet50 to detect which of the following images contain frogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "whIqEWR236Af",
    "outputId": "7a74e30d-310d-4a3a-9ae4-5bf52d137bda"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-bdce712ab92a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install google_images_download'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m         \u001b[0;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2417\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawnb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pexpect-U\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<pexpect factory incomplete>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 303\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptyproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_spawnpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_native_pty_fork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;31m# Use internal fork_pty, for Solaris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pty.py\u001b[0m in \u001b[0;36mfork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmaster_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslave_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHILD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Establish a new session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "!pip install google_images_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "EKnnnM8k38sN",
    "outputId": "59f477e9-0b25-4a38-9678-af24e0176535"
   },
   "outputs": [],
   "source": [
    "from google_images_download import google_images_download\n",
    "\n",
    "response = google_images_download.googleimagesdownload()\n",
    "arguments = {\"keywords\": \"animal pond\", \"limit\": 5, \"print_urls\": True}\n",
    "absolute_image_paths = response.download(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "si5YfNqS50QU"
   },
   "source": [
    "At time of writing at least a few do, but since the Internet changes - it is possible your 5 won't. You can easily verify yourself, and (once you have working code) increase the number of images you pull to be more sure of getting a frog. Your goal is to validly run ResNet50 on the input images - don't worry about tuning or improving the model.\n",
    "\n",
    "*Hint* - ResNet 50 doesn't just return \"frog\". The three labels it has for frogs are: `bullfrog, tree frog, tailed frog`\n",
    "\n",
    "*Stretch goal* - also check for fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaT07ddW3nHz"
   },
   "outputs": [],
   "source": [
    "# TODO - your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEuhvSu7O5Rf"
   },
   "source": [
    "## Part 3 - AutoML\n",
    "\n",
    "Use [TPOT](https://github.com/EpistasisLab/tpot) to fit a predictive model for the King County housing data, with `price` as the target output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "pz0e_7Ve60pM",
    "outputId": "7d7f644d-2edc-417b-bd7e-14d7044ef032"
   },
   "outputs": [],
   "source": [
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "GflK25wp7jnf",
    "outputId": "a3e65568-5bfa-4b33-ca10-da515d8b418d"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "7G5-Gqyg9A-V",
    "outputId": "1140fe22-952d-4745-d628-d91766af65b8"
   },
   "outputs": [],
   "source": [
    "!head kc_house_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KynXZjOY8hBL"
   },
   "source": [
    "As with previous questions, your goal is to run TPOT and successfully run and report error at the end.  Also, in the interest of time, feel free to choose small `generation=1` and `population_size=10` parameters so your pipeline runs efficiently and you are able to iterate and test.\n",
    "\n",
    "*Hint* - you'll have to drop and/or type coerce at least a few variables to get things working. It's fine to err on the side of dropping to get things running, as long as you still get a valid model with reasonable predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOREO8VJO7MZ"
   },
   "outputs": [],
   "source": [
    "# TODO - your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "626zYgjkO7Vq"
   },
   "source": [
    "## Part 4 - More..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__lDWfcUO8oo"
   },
   "source": [
    "Answer the following questions, with a target audience of a fellow Data Scientist:\n",
    "\n",
    "- What do you consider your strongest area, as a Data Scientist?\n",
    "- What area of Data Science would you most like to learn more about, and why?\n",
    "- Where do you think Data Science will be in 5 years?\n",
    "\n",
    "A few sentences per answer is fine - only elaborate if time allows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Hoqe3mM_Mtc"
   },
   "source": [
    "Thank you for your hard work, and congratulations! You've learned a lot, and should proudly call yourself a Data Scientist."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_Unit_4_Sprint_Challenge_4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
