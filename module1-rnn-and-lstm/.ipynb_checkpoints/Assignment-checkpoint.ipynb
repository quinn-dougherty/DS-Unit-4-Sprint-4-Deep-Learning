{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0789266 , -0.0328587 ,  0.1068345 ,  0.05701834, -0.01675757,\n",
       "        -0.05880708, -0.07070989, -0.06798608, -0.07230514,  0.01497346,\n",
       "         0.29373634, -0.18537988, -0.19828053, -0.05705927,  0.01760201,\n",
       "        -0.20139985,  0.02900487,  0.04633741, -0.00915058,  0.03458025,\n",
       "        -0.08848605,  0.13457307,  0.07091436, -0.20683458, -0.04909043],\n",
       "       [ 0.04278544, -0.01996577,  0.06919105,  0.02963511, -0.01909635,\n",
       "        -0.03296782, -0.04729451, -0.01994595, -0.02070541,  0.0021733 ,\n",
       "         0.13404393, -0.07099988, -0.10743214, -0.03705027,  0.02176441,\n",
       "        -0.105198  ,  0.03469446,  0.02480385,  0.00185801,  0.01120837,\n",
       "        -0.03246032,  0.04799902,  0.01738365, -0.10477541, -0.01892862]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request # the lib that handles the url stuff\n",
    "import re\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "# Step 2\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def _dir(obj): \n",
    "    return [k for k in dir(obj) if '_' not in k]\n",
    "\n",
    "def make_corp_src(drop_prefix_trash: int = 66, max_lines: int = 1000) -> List[str]:\n",
    "    with open('bard.txt') as f: \n",
    "        return list(filter(lambda s: s[0] not in ['\\n', '\\r'], [next(f) for x in range(max_lines)]))[drop_prefix_trash:]\n",
    "\n",
    "head = make_corp_src()\n",
    "\n",
    "words = [word_tokenize(text) for text in head]\n",
    "\n",
    "model = Word2Vec(words, min_count=5, size=25)\n",
    "\n",
    "model.wv['love', 'thine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
